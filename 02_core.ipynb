{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from peptide.imports import *\n",
    "from peptide.data import ProteinDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interesting-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "> Learner class and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def visualize_2pcs(pcs, y):\n",
    "    \"\"\"Visualize 2 principal components.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    plot = plt.scatter(pcs[:, 0], pcs[:, 1], c=y, marker=\".\")\n",
    "    ax.set_xlabel(\"PC 1\")\n",
    "    ax.set_ylabel(\"PC 2\")\n",
    "    ax.legend(handles=plot.legend_elements()[0], labels=[\"No\", \"Yes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def visualize_3pcs(pcs, y):\n",
    "    \"\"\"Visualize 3 principal components.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    plot = ax.scatter(pcs[:, 0], pcs[:, 1], pcs[:, 2], c=y)\n",
    "    ax.set_xlabel(\"PC 1\")\n",
    "    ax.set_ylabel(\"PC 2\")\n",
    "    ax.set_zlabel(\"PC 3\")\n",
    "    ax.legend(handles=plot.legend_elements()[0], labels=[\"No\", \"Yes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def train_predict(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Utility helper function to quickly train and predict\"\"\"\n",
    "\n",
    "    lr = LogisticRegression(max_iter=10000)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    svc = LinearSVC(max_iter=10000)\n",
    "    svc.fit(X_train, y_train)\n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "    lr_preds = lr.predict(X_test)\n",
    "    svc_preds = svc.predict(X_test)\n",
    "    xgb_preds = xgb.predict(X_test)\n",
    "\n",
    "    scores = []\n",
    "    for preds in [lr_preds, svc_preds, xgb_preds]:\n",
    "        scores.append(\n",
    "            [\n",
    "                accuracy_score(y_test, preds),\n",
    "                recall_score(y_test, preds),\n",
    "                precision_score(y_test, preds),\n",
    "                f1_score(y_test, preds),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        scores, columns=[\"acc\", \"recall\", \"precision\", \"f1\"], index=[\"lr\", \"svc\", \"xgb\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "- To suppress warning in multithreaded runs - https://github.com/scikit-learn/scikit-learn/issues/12939\n",
    "    - But makes it slow\n",
    "- To revert - delete the following\n",
    "```python\n",
    "from sklearn.utils import parallel_backend\n",
    "..\n",
    "    with parallel_backend(\"multiprocessing\"):\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# from sklearn.utils import parallel_backend\n",
    "\n",
    "\n",
    "class Learner:\n",
    "    \"\"\"Class for training and prediction.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: ProteinDataset,\n",
    "        ohe: bool = False,\n",
    "        pca: bool = True,\n",
    "        pca_n_components: int = 50,\n",
    "    ):\n",
    "        \"\"\"Initialize learner for training and prediction.\"\"\"\n",
    "        self.classifiers = [\"LogisticRegression\", \"LinearSVC\", \"XGBClassifier\"]\n",
    "        self.data = dataset\n",
    "        self.pipeline = self.create_pipeline(\n",
    "            self.data.X_train.shape[1], ohe, pca, pca_n_components\n",
    "        )\n",
    "        self.param_grids = self.create_param_grid()\n",
    "        self.grid_list, self.train_results = [], []\n",
    "        self.predict_results = None\n",
    "\n",
    "    def create_pipeline(\n",
    "        self, num_features: int, ohe: bool, pca: bool, pca_n_components\n",
    "    ) -> Pipeline:\n",
    "        \"\"\"Create and return pipeline\"\"\"\n",
    "\n",
    "        steps = []\n",
    "        if ohe:\n",
    "            steps.append((\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)))\n",
    "        if pca:\n",
    "            steps.append((\"pca\", PCA(n_components=pca_n_components)))\n",
    "        steps.append((\"classifier\", \"passthrough\"))\n",
    "\n",
    "        pipe = Pipeline(steps)\n",
    "\n",
    "        return pipe\n",
    "\n",
    "    def create_param_grid(self) -> list:\n",
    "        \"\"\"Create and return a gird search param grid\"\"\"\n",
    "\n",
    "        lr_grids = [\n",
    "            {\n",
    "                \"classifier\": [LogisticRegression()],\n",
    "                \"classifier__solver\": [\"lbfgs\"],\n",
    "                \"classifier__penalty\": [\"l2\"],\n",
    "                \"classifier__C\": np.logspace(-2, 2, 5),  # default=1.0\n",
    "                \"classifier__max_iter\": [1000, 5000, 10000],  # default=100\n",
    "            },\n",
    "            {\n",
    "                \"classifier\": [LogisticRegression()],\n",
    "                \"classifier__solver\": [\"liblinear\"],\n",
    "                \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "                \"classifier__C\": np.logspace(-2, 2, 5),\n",
    "                \"classifier__max_iter\": [1000, 5000, 10000],\n",
    "            },\n",
    "        ]\n",
    "        svm_grids = [\n",
    "            {\n",
    "                \"classifier\": [LinearSVC()],\n",
    "                \"classifier__loss\": [\"hinge\"],  # default=squared_hinge\n",
    "                \"classifier__penalty\": [\"l2\"],  # default=l2\n",
    "                \"classifier__C\": np.logspace(-2, 2, 5),  # default=1.0\n",
    "                \"classifier__max_iter\": [10000, 20000],  # default=1000\n",
    "            },\n",
    "            {\n",
    "                \"classifier\": [LinearSVC()],\n",
    "                \"classifier__loss\": [\"squared_hinge\"],\n",
    "                \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "                \"classifier__C\": np.logspace(-2, 2, 5),\n",
    "                \"classifier__max_iter\": [10000, 20000],\n",
    "            },\n",
    "        ]\n",
    "        xgb_grid = {\n",
    "            \"classifier\": [XGBClassifier()],\n",
    "            # \"classifier__gamma\": (5, 10),\n",
    "            \"classifier__learning_rate\": np.linspace(0.03, 0.3, 4),  # default 0.1\n",
    "            \"classifier__max_depth\": [3, 4, 5, 6],  # default 6\n",
    "            \"classifier__n_estimators\": [100, 300],  # default 100\n",
    "            # \"classifier__subsample\": (0.5, 1),\n",
    "        }\n",
    "\n",
    "        return [lr_grids, svm_grids, xgb_grid]\n",
    "\n",
    "    def train(\n",
    "        self, scoring: str = \"accuracy\", cv: int = 3, n_jobs: int = -1\n",
    "    ) -> tuple[list, list]:\n",
    "        \"\"\"Run GridSearchCV for all models on X_train and y_train of dataset.\n",
    "\n",
    "        Args:\n",
    "            scoring: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "            cv: defaults to 3-fold cv\n",
    "            n_jobs: defaults to -1 to use all cores\n",
    "        Returns:\n",
    "            result_list: list of grid search results\n",
    "            grid_list: list of grid objects\n",
    "        \"\"\"\n",
    "\n",
    "        result_list = []\n",
    "        grid_list = []\n",
    "\n",
    "        for classifier, param_grid in zip(self.classifiers, self.param_grids):\n",
    "            print(f\"Starting grid search for {classifier}\")\n",
    "            grid = GridSearchCV(\n",
    "                estimator=self.pipeline,\n",
    "                param_grid=param_grid,\n",
    "                n_jobs=n_jobs,\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                verbose=1,\n",
    "            )\n",
    "\n",
    "            # with parallel_backend(\"multiprocessing\"):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                grid.fit(self.data.X_train, self.data.y_train)\n",
    "\n",
    "            result_list.append(pd.DataFrame.from_dict(grid.cv_results_))\n",
    "            grid_list.append(grid)\n",
    "\n",
    "        self.result_list = result_list\n",
    "        self.grid_list = grid_list\n",
    "\n",
    "        return self.result_list, self.grid_list\n",
    "\n",
    "    def get_top_5_train_results(self) -> list:\n",
    "        \"Return top 5 results for each grid\"\n",
    "        results = []\n",
    "        for result in self.result_list:\n",
    "            results.append(result.sort_values(\"rank_test_score\")[:5])\n",
    "        return results\n",
    "\n",
    "    def predict(self) -> pd.DataFrame:\n",
    "        \"\"\"Get predictions on the datasets X_test from best estimators of GridSearchCV.\"\"\"\n",
    "        results = []\n",
    "        for classifier, grid in zip(self.classifiers, self.grid_list):\n",
    "            preds = grid.predict(self.data.X_test)\n",
    "            result = [\n",
    "                classifier,\n",
    "                grid.best_params_,\n",
    "                accuracy_score(self.data.y_test, preds),\n",
    "                recall_score(self.data.y_test, preds),\n",
    "                precision_score(self.data.y_test, preds),\n",
    "                f1_score(self.data.y_test, preds),\n",
    "            ]\n",
    "            results.append(result)\n",
    "\n",
    "        self.predict_results = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\n",
    "                \"classifier\",\n",
    "                \"best_params\",\n",
    "                \"accuracy\",\n",
    "                \"recall\",\n",
    "                \"precision\",\n",
    "                \"f1\",\n",
    "            ],\n",
    "        )\n",
    "        return self.predict_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def visualize_elbow(X: np.ndarray, ks: list) -> None:\n",
    "    \"\"\"Visualize elbow plot for KMeans.\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    inertias = []\n",
    "    for k in ks:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10).fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    plt.plot(ks, inertias)\n",
    "    plt.xticks(ks)\n",
    "    plt.xlabel(\"Number of clusters\")\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.title(\"Elbow plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def plot_silhouette_scores(\n",
    "    max_clusters: int, X: np.ndarray, random_state: int = 10\n",
    ") -> None:\n",
    "    \"\"\"List and plot silhouette scores for KMeans.\"\"\"\n",
    "\n",
    "    silhouette_scores, clusters = [], []\n",
    "\n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        clusters.append(n_clusters)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        print(f\"n_clusters: {n_clusters} -- avg silhouette score: {silhouette_avg}\")\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(clusters, silhouette_scores)\n",
    "    plt.xticks(clusters)\n",
    "    plt.xlabel(\"Number of clusters\")\n",
    "    plt.ylabel(\"Silhouette scores\")\n",
    "    plt.title(\"Silhoutte Scores vs Clusters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def visualize_clusters(clust_lbls: np.ndarray, X: np.ndarray) -> None:\n",
    "    \"\"\"Visualize clusters in a plot of 2 principal components.\"\"\"\n",
    "\n",
    "    X_plot = pd.DataFrame(X[:, :2], columns=[\"pc1\", \"pc2\"])\n",
    "    X_plot[\"cluster_id\"] = clust_lbls\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.scatterplot(\n",
    "        data=X_plot,\n",
    "        x=X_plot[\"pc1\"],\n",
    "        y=X_plot[\"pc2\"],\n",
    "        hue=\"cluster_id\",\n",
    "        palette=\"deep\",\n",
    "        legend=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def pick_k(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    max_clusters: int = 10,\n",
    "    ohe: bool = False,\n",
    "    pca_n_components: int = 50,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Visualize to help determine k for KMeans.\"\"\"\n",
    "\n",
    "    # concat X\n",
    "    X = np.concatenate((X_train, X_test), axis=0)\n",
    "    assert (X_train.shape[0] + X_test.shape[0]) == X.shape[0]\n",
    "    # print(X_train.shape, X_test.shape, X.shape)\n",
    "\n",
    "    if ohe:\n",
    "        # One Hot Encode X\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "        X = ohe.fit_transform(X)\n",
    "        # print(X.shape)\n",
    "\n",
    "    # Dim Reduce X\n",
    "    pca = PCA(n_components=pca_n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # print(X_pca.shape)\n",
    "\n",
    "    # visualize elbow plot\n",
    "    visualize_elbow(X_pca, np.arange(2, max_clusters))\n",
    "    # visualize silhouette scores and plot\n",
    "    plot_silhouette_scores(max_clusters=max_clusters, X=X_pca)\n",
    "\n",
    "    return X_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def analyze_clusters(X: np.ndarray, k: int, random_state: int = 10) -> None:\n",
    "    \"\"\"Analyze clusters identified by KMeans.\"\"\"\n",
    "\n",
    "    km = KMeans(n_clusters=k, random_state=random_state).fit(X)\n",
    "    print(f\"Cluster counts: {Counter(km.labels_)}\")\n",
    "    visualize_clusters(km.labels_, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def run_label_spreading(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    ohe: bool = False,\n",
    "    pca_n_components: int = 50,\n",
    ") -> None:\n",
    "    \"\"\"Run Lanel Spreading and print classification report.\"\"\"\n",
    "\n",
    "    # concat X\n",
    "    X = np.concatenate((X_train, X_test), axis=0)\n",
    "    assert (X_train.shape[0] + X_test.shape[0]) == X.shape[0]\n",
    "    # print(X_train.shape, X_test.shape, X.shape)\n",
    "\n",
    "    # concat y\n",
    "    y = np.concatenate((y_train, np.full(y_test.shape, -1)), axis=0)\n",
    "    assert (y_train.shape[0] + y_test.shape[0]) == y.shape[0]\n",
    "    # print(y_train.shape, y_test.shape, y.shape)\n",
    "\n",
    "    if ohe:\n",
    "        # One Hot Encode X\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "        X_ohe = ohe.fit_transform(X)\n",
    "        # print(X.shape)\n",
    "\n",
    "    # Dim Reduce X\n",
    "    pca = PCA(n_components=pca_n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # print(X_pca.shape)\n",
    "\n",
    "    # Run LableSpreading\n",
    "    lbl_spread = LabelSpreading(kernel=\"knn\", alpha=0.01)\n",
    "    lbl_spread.fit(X_pca, y)\n",
    "    semi_sup_preds = lbl_spread.transduction_[X_train.shape[0] :]\n",
    "    assert semi_sup_preds.shape[0] == X_test.shape[0]\n",
    "\n",
    "    # print result\n",
    "    print(classification_report(y_test, semi_sup_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-announcement",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finite-candle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_basics.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_core.ipynb.\n",
      "Converted 03_onehot.ipynb.\n",
      "Converted 04_lstm.ipynb.\n",
      "Converted 05_transformer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('peptide')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed1fe5d2b8444463e19b00657a175b9cae3e282d841010e10efaed3bb2bffc1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
