---

title: peptide


keywords: fastai
sidebar: home_sidebar

summary: "An ML library for peptide classification using pre-trained embeddings."
description: "An ML library for peptide classification using pre-trained embeddings."
nb_path: "nbs/index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This library demonstrates the performance of a series of classifiers on the task of predicting whether a given amino acid sequence is one or more of the 3 target peptides.<br>
Specifically, this library compares the performance of classic ML classifiers trained on vastly different feature representations (of the amino acid sequences) - ranging from One-Hot embeddings to pre-trained embeddings from large protein language models.</p>
<p><strong>Classification Tasks:</strong> Given a sequence of amino acids, classify whether the resulting peptide is one of the following.</p>
<ul>
<li>Anticancer peptide (ACP)</li>
<li>DNA-binding protein</li>
<li>Antimicrobial peptide (AMP)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="peptide">peptide<a class="anchor-link" href="#peptide"> </a></h3><ol>
<li>Git clone the repo.<ul>
<li><a href="https://github.com/vin00d/peptide.git">https://github.com/vin00d/peptide.git</a></li>
</ul>
</li>
<li>Create a new conda environment using the environment.yml file.<ul>
<li><code>cd peptide</code></li>
<li><code>conda env create -n peptide -f environment.yml</code></li>
</ul>
</li>
<li>To try things out, install this library in editable mode.<ul>
<li><code>pip install -e .</code></li>
</ul>
</li>
</ol>
<h3 id="ProSE">ProSE<a class="anchor-link" href="#ProSE"> </a></h3><p>To create LSTM embeddings ..</p>
<ol>
<li>Clone the ProSE repo<ul>
<li><a href="https://github.com/tbepler/prose.git">https://github.com/tbepler/prose.git</a></li>
</ul>
</li>
<li>Then complete setup instructions for ProSE <a href="https://github.com/tbepler/prose#setup-instructions">detailed on their repo</a>, also summarized below.<ul>
<li>Download pre-trained embedding models.</li>
<li>Create conda environment and install dependencies.</li>
</ul>
</li>
</ol>
<h3 id="ESM">ESM<a class="anchor-link" href="#ESM"> </a></h3><p>To create Transformer embeddings ..</p>
<ol>
<li>Clone the ESM repo<ul>
<li><a href="https://github.com/facebookresearch/esm.git">https://github.com/facebookresearch/esm.git</a></li>
</ul>
</li>
<li>Install ESM and its dependencies <a href="https://github.com/facebookresearch/esm#usage-">detailed on their repo</a>, one option summarized below.<ul>
<li>Create new conda environment with python 3.9</li>
<li>In that conda environment, pip install <code>torch</code> and <code>fair-esm==0.5.0</code></li>
</ul>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-use">How to use<a class="anchor-link" href="#How-to-use"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Read through any (or all) of the following quick start guides to get a general idea. Then try running any of them as detailed below:</p>
<ul>
<li>Option 1: run any of the following jupyter noteboooks in the <code>nbs</code> folder.<ul>
<li><code>03_onehot.ipynb</code></li>
<li><code>04_lstm.ipynb</code></li>
<li><code>05_transformer.ipynb</code></li>
</ul>
</li>
<li>Option 2: just open a jupyter notebook and copy, paste &amp; run cell-by-cell from any of the following quick start guides.<ul>
<li><a href="https://vin00d.github.io/peptide/onehot.html">Onehot Embeddings</a></li>
<li><a href="https://vin00d.github.io/peptide/lstm.html">LSTM Embeddings</a></li>
<li><a href="https://vin00d.github.io/peptide/transformer.html">Transformer Embeddings</a></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Settings-file-and-DATASTORE">Settings file and DATASTORE<a class="anchor-link" href="#Settings-file-and-DATASTORE"> </a></h3><ul>
<li>The steps demonstrated in these notebooks use default locations for datastore, etc as detailed in <a href="https://vin00d.github.io/peptide/basics.html">Basics</a>.</li>
<li>The first cell in every notebook imports these settings for convenience.</li>
<li>So if you intend to use default settings, make sure to place the datasets in the DATASTORE as detailed next.</li>
</ul>
<p><strong>Note:</strong> The settings file and default folder structure will be created by either executing <code>from peptide.basics import *</code> in a cell or executing the first cell in any of the above notebooks. This will create a <code>DATASTORE</code> variable pointing to the path <code>~/.peptide/datasets</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Copy-Datasets-Into-DATASTORE">Copy Datasets Into DATASTORE<a class="anchor-link" href="#Copy-Datasets-Into-DATASTORE"> </a></h3><ul>
<li>Copy dataset directories into the location pointed to by the <code>DATASTORE</code> global variable.<ul>
<li>for example <code>~/.peptide/datasets</code></li>
</ul>
</li>
<li>Resulting folder structure <strong>must</strong> be<ul>
<li><code>~/.peptide/datasets/acp/train_data.csv</code></li>
<li><code>~/.peptide/datasets/amp/all_data.csv</code></li>
<li><code>~/.peptide/datasets/dna_binding/train.csv</code></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This library is created using the awesome <a href="https://nbdev1.fast.ai/">nbdev v1</a>, soon to be upgraded to <a href="https://www.fast.ai/2022/07/28/nbdev-v2/">nbdev v2</a>.</p>
<p>Pretrained embeddings used in this library are from the following papers</p>
<ol>
<li>LSTM - Protein Sequence Embeddings (<strong>ProSE</strong>) - Multi-task and masked language model-based protein sequence embedding models - <a href="https://github.com/tbepler/prose">GitHub</a><blockquote><p>Bepler, T., Berger, B. Learning the protein language:evolution, structure, and function. Cell Systems 12, 6 (2021). <a href="https://doi.org/10.1016/j.cels.2021.05.017&gt;">https://doi.org/10.1016/j.cels.2021.05.017&gt;</a> Bepler, T., Berger, B. Learning protein sequence embeddings using information from structure. International Conference on Learning Representations (2019). <a href="https://openreview.net/pdf?id=SygLehCqtm">https://openreview.net/pdf?id=SygLehCqtm</a></p>
</blockquote>
</li>
<li>Transformer - Evolutionary Scale Modeling (<strong>ESM</strong>) - <a href="https://github.com/facebookresearch/esm">GitHub</a><blockquote><p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, Rob Fergus
bioRxiv 622803; doi:<a href="https://doi.org/10.1101/622803">https://doi.org/10.1101/622803</a></p>
</blockquote>
</li>
</ol>

</div>
</div>
</div>
</div>
 

