---

title: peptide


keywords: fastai
sidebar: home_sidebar

summary: "An open source library for peptide classification using Machine Learning and Deep Learning."
description: "An open source library for peptide classification using Machine Learning and Deep Learning."
nb_path: "nbs/index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This project aims to build a series of classifiers that can predict whether a given amino acid sequence is one or more of the 3 target peptides.<br>
Additionally, this project will compare models built with vastly different approaches ranging from classic ML models with feature engineering by experts to large transformer-based models.</p>
<p><strong>Classification Tasks:</strong> Given a sequence of amino acids (NLP equivalent = ‘words’), classify whether the resulting peptide (NLP equivalent = ‘sentence’) is one or many of the following targets (multi-task classification not multi-class i.e. 3 sigmoids not 1 softmax).</p>
<ul>
<li>Anticancer peptide (ACP)</li>
<li>DNA-binding protein</li>
<li>Antimicrobial peptide (AMP).</li>
</ul>
<p><strong>Multiple Models:</strong> Develop multiple models for the above classification tasks</p>
<ul>
<li>Supervised ML model(s) with hand engineered features from bio-experts</li>
<li>Supervised ML model(s) using pre-trained embeddings with feature-space reduced using the following unsupervised techniques<ul>
<li>Principal Component Analysis (PCA)</li>
<li>Autoencoders &amp; its variants</li>
<li>K-Means Clustering</li>
</ul>
</li>
<li>Deep Learning Transformer models using pre-trained embeddings</li>
</ul>
<p><strong>Model Scoring System:</strong> Develop a system that produces accuracy scores</p>
<ul>
<li>By running any given (labeled) dataset</li>
<li>Through the multiple models listed above</li>
<li>And display / return accuracies on the classification tasks for each model.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With conda</p>
<ul>
<li><code>conda install -c conda-forge peptide</code></li>
</ul>
<p>With pip</p>
<ul>
<li><code>pip install peptide</code></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-use">How to use<a class="anchor-link" href="#How-to-use"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>git clone this repo</li>
<li>create conda env - <code>conda env create -n peptide -f environment.yml</code></li>
<li><code>conda activate peptide</code></li>
</ul>
<p>To develop using nbs ..</p>
<ul>
<li><code>pip install -e .</code></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Fill me in please! Don't forget code examples:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="mi">1</span><span class="o">+</span><span class="mi">1</span>
</pre></div>
</body></html>


    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This library is created using the awesome <a href="https://nbdev1.fast.ai/">nbdev v1</a>, soon to be upgraded to <a href="https://www.fast.ai/2022/07/28/nbdev-v2/">nbdev v2</a>.</p>
<p>Pretrained embeddings used in this library are from the following papers</p>
<ol>
<li>LSTM - Protein Sequence Embeddings (<strong>ProSE</strong>) - Multi-task and masked language model-based protein sequence embedding models - <a href="https://github.com/tbepler/prose">GitHub</a></li>
</ol>
<blockquote>
<p>Bepler, T., Berger, B. Learning the protein language:evolution, structure, and function. Cell Systems 12, 6 (2021). https://doi.org/10.1016/j.cels.2021.05.017&gt; Bepler, T., Berger, B. Learning protein sequence embeddings using information from structure. International Conference on Learning Representations (2019). https://openreview.net/pdf?id=SygLehCqtm</p>
</blockquote>
<ol start="2">
<li>Transformer - Evolutionary Scale Modeling (<strong>ESM</strong>) - <a href="https://github.com/facebookresearch/esm">GitHub</a></li>
</ol>
<blockquote>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, Rob Fergus</p>
</blockquote>
<pre><code>&gt; bioRxiv 622803; doi:https://doi.org/10.1101/622803
</code></pre>

</div>
</div>
</div>
</div>


