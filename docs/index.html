---

title: peptide


keywords: fastai
sidebar: home_sidebar

summary: "An ML library for peptide classification using pre-trained embeddings."
description: "An ML library for peptide classification using pre-trained embeddings."
nb_path: "nbs/index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This project aims to build a series of classifiers that can predict whether a given amino acid sequence is one or more of the 3 target peptides.<br>
Additionally, this project will compare models built with vastly different approaches ranging from classic ML models with feature engineering by experts to large transformer-based models.</p>
<p><strong>Classification Tasks:</strong> Given a sequence of amino acids (NLP equivalent = ‘words’), classify whether the resulting peptide (NLP equivalent = ‘sentence’) is one or many of the following targets (multi-task classification not multi-class i.e. 3 sigmoids not 1 softmax).</p>
<ul>
<li>Anticancer peptide (ACP)</li>
<li>DNA-binding protein</li>
<li>Antimicrobial peptide (AMP).</li>
</ul>
<p><strong>Multiple Models:</strong> Develop multiple models for the above classification tasks</p>
<ul>
<li>Supervised ML model(s) with hand engineered features from bio-experts</li>
<li>Supervised ML model(s) using pre-trained embeddings with feature-space reduced using the following unsupervised techniques<ul>
<li>Principal Component Analysis (PCA)</li>
<li>Autoencoders &amp; its variants</li>
<li>K-Means Clustering</li>
</ul>
</li>
<li>Deep Learning Transformer models using pre-trained embeddings</li>
</ul>
<p><strong>Model Scoring System:</strong> Develop a system that produces accuracy scores</p>
<ul>
<li>By running any given (labeled) dataset</li>
<li>Through the multiple models listed above </li>
<li>And display / return accuracies on the classification tasks for each model.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="peptide">peptide<a class="anchor-link" href="#peptide"> </a></h3><ol>
<li>Git clone the repo.<ul>
<li><a href="https://github.com/vin00d/peptide.git">https://github.com/vin00d/peptide.git</a></li>
</ul>
</li>
<li>Create a new conda env using the environment.yml file.<ul>
<li><code>cd peptide</code></li>
<li><code>conda env create -n peptide -f environment.yml</code></li>
</ul>
</li>
<li>To try things out, install this library in editable mode.<ul>
<li><code>pip install -e .</code></li>
</ul>
</li>
</ol>
<h3 id="ProSE">ProSE<a class="anchor-link" href="#ProSE"> </a></h3><p>To create LSTM embeddings ..</p>
<ol>
<li>Clone the ProSE repo<ul>
<li><a href="https://github.com/tbepler/prose.git">https://github.com/tbepler/prose.git</a></li>
</ul>
</li>
<li>Then complete setup instructions for ProSE <a href="https://github.com/tbepler/prose#setup-instructions">detailed on their repo</a>, also summarized below.<ul>
<li>Download pre-trained embedding models.</li>
<li>Create conda env and install dependencies.</li>
</ul>
</li>
</ol>
<h3 id="ESM">ESM<a class="anchor-link" href="#ESM"> </a></h3><p>To create Transformer embeddings ..</p>
<ol>
<li>Clone the ESM repo<ul>
<li><a href="https://github.com/facebookresearch/esm.git">https://github.com/facebookresearch/esm.git</a></li>
</ul>
</li>
<li>Install ESM and its dependesncies <a href="https://github.com/facebookresearch/esm#usage-">detailed on their repo</a>, one option summarized below.<ul>
<li>Create new conda env with python 3.9</li>
<li>In that conda env pip install <code>torch</code> and <code>fair-esm==0.5.0</code></li>
</ul>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-use">How to use<a class="anchor-link" href="#How-to-use"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>Read through the quick start guides and run cell by cell</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This library is created using the awesome <a href="https://nbdev1.fast.ai/">nbdev v1</a>, soon to be upgraded to <a href="https://www.fast.ai/2022/07/28/nbdev-v2/">nbdev v2</a>.</p>
<p>Pretrained embeddings used in this library are from the following papers</p>
<ol>
<li>LSTM - Protein Sequence Embeddings (<strong>ProSE</strong>) - Multi-task and masked language model-based protein sequence embedding models - <a href="https://github.com/tbepler/prose">GitHub</a><blockquote><p>Bepler, T., Berger, B. Learning the protein language:evolution, structure, and function. Cell Systems 12, 6 (2021). <a href="https://doi.org/10.1016/j.cels.2021.05.017&gt;">https://doi.org/10.1016/j.cels.2021.05.017&gt;</a> Bepler, T., Berger, B. Learning protein sequence embeddings using information from structure. International Conference on Learning Representations (2019). <a href="https://openreview.net/pdf?id=SygLehCqtm">https://openreview.net/pdf?id=SygLehCqtm</a></p>
</blockquote>
</li>
<li>Transformer - Evolutionary Scale Modeling (<strong>ESM</strong>) - <a href="https://github.com/facebookresearch/esm">GitHub</a><blockquote><p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, Rob Fergus
bioRxiv 622803; doi:<a href="https://doi.org/10.1101/622803">https://doi.org/10.1101/622803</a></p>
</blockquote>
</li>
</ol>

</div>
</div>
</div>
</div>
 

