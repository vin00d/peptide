# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_data.ipynb (unless otherwise specified).

__all__ = ['ProteinDataset', 'ACPDataset', 'AMPDataset', 'DNABindDataset', 'plot_seqlen_dist', 'plot_AA_dist']

# Cell

from .basics import *
from .imports import *


# Cell
class ProteinDataset(ABC):
    """Abstract base class for protein datasets."""

    def __init__(
        self,
        name: str,  # name of the dataset (e.g. 'acp' or 'amp' or 'dna')
        location: str,  # location of the raw dataset
        max_seq_len: int = None,  # amino acid sequences will be truncated at this max length
    ):
        self.name = name
        self.location = location
        self.max_seq_len = max_seq_len

    @abstractmethod
    def clean_data(self):
        """Abstract method for cleaning raw data."""
        pass

    def extract_features_labels(
        self, df: pd.DataFrame  # cleaned train or test dataframe
    ) -> tuple[pd.DataFrame, np.ndarray, np.ndarray]:
        """Extract features and separate labels."""

        df["seq_list"] = df["sequence"].apply(lambda x: list(x))
        df["length"] = df["seq_list"].apply(lambda x: len(x))

        features = pd.DataFrame(df["seq_list"].to_list())
        if self.max_seq_len:
            features = features.loc[:, : self.max_seq_len - 1]
        features = features.to_numpy()

        labels = np.ravel(df["label"].to_numpy())

        df.drop(columns=["seq_list"], inplace=True)

        return df, features, labels

    def generate_fasta_files(
        self,
        out_dir: str = None,  # output directory, defaults to 'dataset_location/dataset_name/fasta'
        use_seq_max_len: bool = False,  # if True, uses truncated amino acid sequences, else full sequence
    ) -> None:
        """Generate fasta files (using the BioPython lib) for the given protein dataset."""

        ds_name = self.name
        location = (
            Path(out_dir) if out_dir else Path(f"{self.location}/{ds_name}/fasta")
        )
        location.mkdir(parents=True, exist_ok=True)

        for df, split in zip([self.train, self.test], ["train", "test"]):
            seq_recs = []
            for i in range(len(df)):
                seq, lbl = df.loc[i, ["sequence", "label"]]
                seq_recs.append(
                    SeqRecord(
                        Seq(seq),
                        id=str(i),
                        description=f"|{lbl}",
                    )
                )

            # truncate sequences or not
            if use_seq_max_len:
                seq_recs = [seq_rec[: self.max_seq_len] for seq_rec in seq_recs]
                file_name = (
                    f"{location}/{ds_name}_{split}_seqlen_{self.max_seq_len}.fasta"
                )
            else:
                file_name = f"{location}/{ds_name}_{split}.fasta"

            # write fasta file
            with open(file_name, "w") as output_handle:
                SeqIO.write(seq_recs, output_handle, "fasta")

            # print counts
            print(f"Created {file_name} with {len(seq_recs)} sequence records")
        return

    def get_lstm_emb(
        self,
        train_h5_file: str,  # train h5 filename
        test_h5_file: str,  # test h5 filename
        h5_location: str = None,  # LSTM embedding directory - defaults to 'dataset_location/dataset_name/lstm'
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """Read (ProSE) LSTM embeddings from HDF5 files, returns X_train, y_train, X_test, y_test."""

        def _get_emb(h5_file):
            Xs = []
            ys = []
            with h5py.File(h5_file, "r") as f:
                for key in f.keys():
                    label = key.split("|")[-1]
                    ys.append(int(label))
                    seq = f[key][()]
                    Xs.append(seq)
            Xs = np.stack(Xs, axis=0)
            ys = np.stack(ys, axis=0)
            return Xs, ys

        h5_path = (
            Path(h5_location)
            if h5_location
            else Path(f"{self.location}/{self.name}/lstm")
        )
        train_file = Path(f"{h5_path}/{train_h5_file}")
        test_file = Path(f"{h5_path}/{test_h5_file}")

        X_train, y_train = _get_emb(train_file)
        X_test, y_test = _get_emb(test_file)

        return X_train, y_train, X_test, y_test

    def get_transformer_emb(
        self,
        train_fasta_file: str,  # train fasta filename
        test_fasta_file: str,  # test fasta filename
        fasta_location: str = None,  # fasta directory - defaults to 'dataset_location/dataset_name/fasta'
        emb_location: str = None,  # ESM embedding directory - defaults to 'dataset_location/dataset_name/transformer'
        emb_layer: int = 33,  # Layer of ESM Transformer to get embedding from - defaults to final layer (33)
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """Read (ESM) Transformer embeddings."""

        def _get_emb(fasta_file, emb_path, emb_layer):
            ys = []
            Xs = []
            for header, _seq in esm.data.read_fasta(fasta_file):
                label = header.split("|")[-1]
                ys.append(int(label))
                emb_file = f"{emb_path}/{header[1:]}.pt"
                embs = torch.load(emb_file)
                Xs.append(embs["mean_representations"][emb_layer])
            Xs = np.stack(Xs, axis=0)
            ys = np.stack(ys, axis=0)
            return Xs, ys

        fasta_path = (
            Path(fasta_location)
            if fasta_location
            else Path(f"{self.location}/{self.name}/fasta")
        )
        train_fasta = Path(f"{fasta_path}/{train_fasta_file}")
        test_fasta = Path(f"{fasta_path}/{test_fasta_file}")
        emb_path = (
            Path(emb_location)
            if emb_location
            else Path(f"{self.location}/{self.name}/transformer")
        )
        train_emb = Path(f"{emb_path}/train")
        test_emb = Path(f"{emb_path}/test")

        X_train, y_train = _get_emb(train_fasta, train_emb, emb_layer)
        X_test, y_test = _get_emb(test_fasta, test_emb, emb_layer)

        return X_train, y_train, X_test, y_test


# Cell
class ACPDataset(ProteinDataset):
    """Class for Anticancer Peptide Dataset."""

    def __init__(
        self,
        location: str,  # location of raw ACP dataset (must contain dir called 'acp' with train and test CSVs in it)
        max_seq_len: int = None,  # max length, if amino acid sequences are to be truncated
    ):
        """Load, clean, extract labels & features for ACP train and test"""
        super().__init__("acp", location, max_seq_len)

        train_df, test_df = self.clean_data()
        self.train, self.X_train, self.y_train = self.extract_features_labels(train_df)
        self.test, self.X_test, self.y_test = self.extract_features_labels(test_df)

    def clean_data(self) -> tuple[pd.DataFrame, pd.DataFrame]:
        """Implemenation of abstract method - load, clean and return ACP train and test dataframes."""
        acp_train_df = pd.read_csv(f"{self.location}/acp/train_data.csv")
        acp_test_df = pd.read_csv(f"{self.location}/acp/test_data.csv")

        acp_train_df.rename(columns={"sequences": "sequence"}, inplace=True)
        acp_test_df.rename(columns={"sequences": "sequence"}, inplace=True)

        return acp_train_df, acp_test_df


# Cell
class AMPDataset(ProteinDataset):
    """Class for Antimicrobial Peptide Dataset."""

    def __init__(
        self,
        location: str,  # location of raw AMP dataset (must contain dir called 'amp' with 'all_data.csv' in it)
        max_seq_len: int = 150,  # max length that amino acid sequences will be truncated at
        test_pct: float = 0.2,  # split percent for test data
        seed=1234,  # random state for split
    ):
        """Load, clean, extract labels & features for AMP train and test."""
        super().__init__("amp", location, max_seq_len)
        self.test_pct = test_pct
        self.seed = seed

        train_df, test_df = self.clean_data(self.test_pct, self.seed)
        self.train, self.X_train, self.y_train = self.extract_features_labels(train_df)
        self.test, self.X_test, self.y_test = self.extract_features_labels(test_df)

    def clean_data(
        self,
        test_pct: float = 0.2,  # split percent for test data
        seed: int = 1234,  # random state for split
    ) -> tuple[pd.DataFrame, pd.DataFrame]:
        """Implemenation of abstract method - load, clean, split and return AMP train and test dataframes."""
        amp_df = pd.read_csv(f"{self.location}/amp/all_data.csv")

        amp_df.drop(columns=["PDBs_code"], inplace=True)
        amp_df.rename(columns={"SequenceID": "sequence"}, inplace=True)

        amp_test_df = amp_df.sample(frac=test_pct, random_state=seed)
        amp_train_df = amp_df.drop(amp_test_df.index)

        amp_train_df.reset_index(inplace=True)
        amp_test_df.reset_index(inplace=True)

        return amp_train_df, amp_test_df


# Cell
class DNABindDataset(ProteinDataset):
    """Class for DNA Binding Protein Dataset."""

    def __init__(
        self,
        location: str,  # location of raw DNA dataset (must contain dir called 'dna' with train and test CSVs in it)
        max_seq_len: int = 300  # max length that amino acid sequences will be truncated at
    ):
        """Load, clean, extract labels & features for DNA Binding train and test."""
        super().__init__("dna", location, max_seq_len)

        train_df, test_df = self.clean_data()
        self.train, self.X_train, self.y_train = self.extract_features_labels(train_df)
        self.test, self.X_test, self.y_test = self.extract_features_labels(test_df)

    def clean_data(self) -> tuple[pd.DataFrame, pd.DataFrame]:
        """Implemenation of abstract method - load, clean and return DNA Binding train and test dataframes."""

        dna_bind_train_df = pd.read_csv(f"{self.location}/dna/train.csv")
        dna_bind_test_df = pd.read_csv(f"{self.location}/dna/test.csv")

        dna_bind_train_df.drop(columns=["code", "origin"], inplace=True)
        dna_bind_test_df.drop(columns=["code", "origin"], inplace=True)

        return dna_bind_train_df, dna_bind_test_df


# Cell


def plot_seqlen_dist(
    df: pd.DataFrame, dataset_name: str, log_scale: bool = False, bins: int = 75
):
    """Plot the sequence length distribution given a train df with length and label columns."""

    len_col, label_col = df.columns
    mean = df[len_col].mean()
    median = df[len_col].median()
    sorted_by_len = df.sort_values(by=[len_col])

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    fig.suptitle(f'Distribution of sequence lengths in "train" - {dataset_name}')

    # overall dist with mean and median
    sns.histplot(
        data=sorted_by_len[len_col].values,
        kde=True,
        ax=ax1,
        log_scale=log_scale,
        bins=bins,
    )

    ax1.axvline(mean, color="r", label=f"Mean: {mean:.1f}")
    ax1.axvline(median, color="g", label=f"Median {median:.1f}")
    ax1.legend()

    # dist with labels
    sns.histplot(
        data=sorted_by_len,
        x=len_col,
        hue=label_col,
        kde=True,
        ax=ax2,
        log_scale=log_scale,
        bins=bins,
    )

    for ax in [ax1, ax2]:
        ax.set(xlabel="length")

    plt.show()


# Cell


def plot_AA_dist(df: pd.DataFrame, dataset_name: str):
    """Plot the distribution of amino acids given a train df with "sequence" column."""

    aa_counter = Counter()
    for seq in df.sequence:
        aa_counter.update(seq)
    print(f"Number of amino acids in the dataset: {len(aa_counter.keys())}")
    print(f"Frequencies: {aa_counter.most_common()}")
    AAs, counts = map(list, zip(*aa_counter.most_common()))  # to sort

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.set(
        title=f'Frequency distribution of amino acids in {dataset_name} "train" dataset'
    )
    sns.barplot(x=AAs, y=counts, ax=ax)
    ax.set(xlabel="Amino Acids")

    plt.show()
