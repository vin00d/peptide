{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.pretrained.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from peptide.basics import *\n",
    "from peptide.preprocessing.data import (\n",
    "    ProteinDataset,\n",
    "    ACPDataset,\n",
    "    AMPDataset,\n",
    "    DNABindDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/vinod/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm1b_t33_650M_UR50S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-1b model\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]\n",
    "\n",
    "# Generate per-sequence representations via averaging\n",
    "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "sequence_representations = []\n",
    "for i, (_, seq) in enumerate(data):\n",
    "    sequence_representations.append(token_representations[i, 1 : len(seq) + 1].mean(0))\n",
    "\n",
    "# Look at the unsupervised self-attention map contact predictions\n",
    "# import matplotlib.pyplot as plt\n",
    "# for (_, seq), attention_contacts in zip(data, results[\"contacts\"]):\n",
    "#     plt.matshow(attention_contacts[: len(seq), : len(seq)])\n",
    "#     plt.title(seq)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26060754, -0.09024662,  0.09138174, ..., -0.04813125,\n",
       "       -0.16790868, -0.02575146], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_representations[0, 1:4].mean(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1280])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_representations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp_data = ACPDataset(DATA_STORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acp_data.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    (str(i), \"\".join(df.loc[i, \"sequence\"]))\n",
    "        for i in range(len(df))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 'RRWWRRWRRW'),\n",
       " ('1', 'GWKSVFRKAKKVGKTVGGLALDHYLG'),\n",
       " ('2', 'ALWKTMLKKLGTMALHAGKAALGAAADTISQGTQ'),\n",
       " ('3', 'GLFDVIKKVAAVIGGL'),\n",
       " ('4', 'VAKLLAKLAKKVL')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1378"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp_data_sample = [\n",
    "('0', 'RRWWRRWRRW'),\n",
    "('1', 'GWKSVFRKAKKVGKTVGGLALDHYLG'),\n",
    "('2', 'ALWKTMLKKLGTMALHAGKAALGAAADTISQGTQ'),\n",
    "('3', 'GLFDVIKKVAAVIGGL'),\n",
    "('4', 'VAKLLAKLAKKVL')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_labels, batch_strs, batch_tokens = batch_converter(sequences[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 50, 1280])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_representations = []\n",
    "for i, (_, seq) in enumerate(data):\n",
    "    sequence_representations.append(token_representations[i, 1 : len(seq) + 1].mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1280])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_representations[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk from fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACP**\n",
    "```\n",
    "python scripts/extract.py esm1b_t33_650M_UR50S ~/.peptide/datasets/fasta/ACPDataset_train.fasta  ~/.peptide/datasets/transformer/mean/acp/train/ \\    \n",
    "    --repr_layers 33 --include mean\n",
    "Transferred model to GPU\n",
    "Read /home/vinod/.peptide/datasets/fasta/ACPDataset_train.fasta with 1378 sequences\n",
    "Processing 1 of 10 batches (292 sequences)\n",
    "Processing 2 of 10 batches (215 sequences)\n",
    "Processing 3 of 10 batches (178 sequences)\n",
    "Processing 4 of 10 batches (157 sequences)\n",
    "Processing 5 of 10 batches (132 sequences)\n",
    "Processing 6 of 10 batches (117 sequences)\n",
    "Processing 7 of 10 batches (105 sequences)\n",
    "Processing 8 of 10 batches (91 sequences)\n",
    "Processing 9 of 10 batches (80 sequences)\n",
    "Processing 10 of 10 batches (11 sequences)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AMP**\n",
    "```\n",
    "python scripts/extract.py esm1b_t33_650M_UR50S ~/.peptide/datasets/fasta/AMPDataset_test.fasta  ~/.peptide/datasets/transformer/mean/amp/test \\\n",
    "    --repr_layers 33 --include mean\n",
    "Transferred model to GPU\n",
    "Read /home/vinod/.peptide/datasets/fasta/AMPDataset_test.fasta with 808 sequences\n",
    "Processing 1 of 9 batches (204 sequences)\n",
    "Processing 2 of 9 batches (157 sequences)\n",
    "Processing 3 of 9 batches (124 sequences)\n",
    "Processing 4 of 9 batches (102 sequences)\n",
    "Processing 5 of 9 batches (85 sequences)\n",
    "Processing 6 of 9 batches (63 sequences)\n",
    "Processing 7 of 9 batches (44 sequences)\n",
    "Processing 8 of 9 batches (26 sequences)\n",
    "Processing 9 of 9 batches (3 sequences)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From ESM Example\n",
    "https://github.com/facebookresearch/esm/blob/main/examples/sup_variant_prediction.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDRegressor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTA_PATH = \"/home/vinod/.peptide/datasets/fasta/ACPDataset_train.fasta\"\n",
    "EMB_PATH = \"/home/vinod/.peptide/datasets/transformer/mean/acp/train/\"\n",
    "EMB_LAYER = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1378\n",
      "(1378, 1280)\n"
     ]
    }
   ],
   "source": [
    "ys = []\n",
    "Xs = []\n",
    "for header, _seq in esm.data.read_fasta(FASTA_PATH):\n",
    "    scaled_effect = header.split('|')[-1]\n",
    "    ys.append(float(scaled_effect))\n",
    "    fn = f'{EMB_PATH}/{header[1:]}.pt'\n",
    "    embs = torch.load(fn)\n",
    "    Xs.append(embs['mean_representations'][EMB_LAYER])\n",
    "Xs = torch.stack(Xs, dim=0).numpy()\n",
    "print(len(ys))\n",
    "print(Xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.33877844e-02,  2.63488621e-01,  1.12979829e-01, ...,\n",
       "        -2.34928466e-02,  6.65202513e-02, -4.78385836e-01],\n",
       "       [-6.99847043e-02,  2.43899822e-02,  2.28446126e-02, ...,\n",
       "        -1.91894248e-02,  1.02544196e-01, -1.25279754e-01],\n",
       "       [-7.93628395e-02,  4.35871184e-02,  8.71239454e-02, ...,\n",
       "        -7.35687208e-04,  7.49264136e-02,  2.10546394e-04],\n",
       "       ...,\n",
       "       [ 2.17704233e-02,  6.97988719e-02, -3.53524983e-02, ...,\n",
       "         5.78941405e-02,  9.20846909e-02,  7.61147439e-02],\n",
       "       [ 2.50619035e-02,  1.39694765e-01,  1.69001684e-01, ...,\n",
       "        -9.01004970e-02,  6.60669580e-02,  2.08521653e-02],\n",
       "       [ 1.00932933e-01, -1.11178057e-02,  2.40505397e-01, ...,\n",
       "         1.17194327e-02,  2.81297654e-01, -2.07881164e-02]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('peptide')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed1fe5d2b8444463e19b00657a175b9cae3e282d841010e10efaed3bb2bffc1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
