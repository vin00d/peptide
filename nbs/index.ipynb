{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#from your_lib.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# peptide\n",
    "\n",
    "> An open source library for peptide classification using Machine Learning and Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to build a series of classifiers that can predict whether a given amino acid sequence is one or more of the 3 target peptides.<br>\n",
    "Additionally, this project will compare models built with vastly different approaches ranging from classic ML models with feature engineering by experts to large transformer-based models.\n",
    "\n",
    "**Classification Tasks:** Given a sequence of amino acids (NLP equivalent = ‘words’), classify whether the resulting peptide (NLP equivalent = ‘sentence’) is one or many of the following targets (multi-task classification not multi-class i.e. 3 sigmoids not 1 softmax).\n",
    "- Anticancer peptide (ACP)\n",
    "- DNA-binding protein\n",
    "- Antimicrobial peptide (AMP).\n",
    "\n",
    "**Multiple Models:** Develop multiple models for the above classification tasks\n",
    "- Supervised ML model(s) with hand engineered features from bio-experts\n",
    "- Supervised ML model(s) using pre-trained embeddings with feature-space reduced using the following unsupervised techniques\n",
    "    - Principal Component Analysis (PCA)\n",
    "    - Autoencoders & its variants\n",
    "    - K-Means Clustering\n",
    "- Deep Learning Transformer models using pre-trained embeddings\n",
    "\n",
    "**Model Scoring System:** Develop a system that produces accuracy scores \n",
    "- By running any given (labeled) dataset\n",
    "- Through the multiple models listed above \n",
    "- And display / return accuracies on the classification tasks for each model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With conda\n",
    "- `conda install -c conda-forge peptide`\n",
    "\n",
    "With pip\n",
    "- `pip install peptide`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop using nbs ..\n",
    "- `pip install -e .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill me in please! Don't forget code examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library is created using the awesome [nbdev v1](https://nbdev1.fast.ai/), soon to be upgraded to [nbdev v2](https://www.fast.ai/2022/07/28/nbdev-v2/).\n",
    "\n",
    "Pretrained embeddings used in this library are from the following papers\n",
    "\n",
    "1. LSTM - Protein Sequence Embeddings (**ProSE**) - Multi-task and masked language model-based protein sequence embedding models - [GitHub](https://github.com/tbepler/prose)\n",
    "    > Bepler, T., Berger, B. Learning the protein language: evolution, structure, and function. Cell Systems 12, 6 (2021). https://doi.org/10.1016/j.cels.2021.05.017\n",
    "    \n",
    "    > Bepler, T., Berger, B. Learning protein sequence embeddings using information from structure. International Conference on Learning Representations (2019). https://openreview.net/pdf?id=SygLehCqtm\n",
    "\n",
    "2. Transformer - Evolutionary Scale Modeling (**ESM**) - [GitHub](https://github.com/facebookresearch/esm)\n",
    "    > Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, Rob Fergus\n",
    "    > bioRxiv 622803; doi: https://doi.org/10.1101/622803\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('daily')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
