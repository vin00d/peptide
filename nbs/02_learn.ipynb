{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e29651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from peptide.imports import *\n",
    "from peptide.utils import *\n",
    "from peptide.data import ProteinDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn\n",
    "> Learner class for supervised, unsupervised and semi supervised learning with Protein data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "- To suppress warning in multithreaded runs - https://github.com/scikit-learn/scikit-learn/issues/12939\n",
    "    - But makes it slow\n",
    "- To revert - delete the following\n",
    "```python\n",
    "from sklearn.utils import parallel_backend\n",
    "..\n",
    "    with parallel_backend(\"multiprocessing\"):\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# from sklearn.utils import parallel_backend\n",
    "\n",
    "\n",
    "class Learner:\n",
    "    \"\"\"Class for training and prediction.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_train: np.ndarray,  # X_train numpy ndarray\n",
    "        y_train: np.ndarray,  # y_train numpy ndarray\n",
    "        X_test: np.ndarray,  # X_test numpy ndarray\n",
    "        y_test: np.ndarray,  # y_test numpy ndarray\n",
    "        ohe: bool = False,  # to use one hot encoding or not\n",
    "        scaler: bool = False,  # to use standard scaling or not\n",
    "        pca: bool = True,  # to use principal component analysis or not\n",
    "        pca_n_components: int = 50,  # PCA number of components\n",
    "        param_grids: list = None,  # param_grid for grid search, if None - gets default grid from utils\n",
    "    ):\n",
    "        \"\"\"Initialize learner for training and prediction.\"\"\"\n",
    "        self.classifiers = [\"LogisticRegression\", \"LinearSVC\", \"XGBClassifier\"]\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        self.ohe, self.scaler, self.pca = ohe, scaler, pca\n",
    "        self.pca_n_components = pca_n_components\n",
    "        self.pipeline = self.create_pipeline()\n",
    "        self.param_grids = (\n",
    "            get_default_param_grid() if param_grids is None else param_grids\n",
    "        )\n",
    "\n",
    "        self.grid_list, self.train_results = [], []\n",
    "        self.predict_results = None\n",
    "\n",
    "    def create_pipeline(self) -> Pipeline:\n",
    "        \"\"\"Create and return pipeline\"\"\"\n",
    "\n",
    "        steps = []\n",
    "        if self.ohe:\n",
    "            steps.append((\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)))\n",
    "        if self.scaler:\n",
    "            steps.append((\"scaler\", StandardScaler()))\n",
    "        if self.pca:\n",
    "            steps.append((\"pca\", PCA(n_components=self.pca_n_components)))\n",
    "        steps.append((\"classifier\", \"passthrough\"))\n",
    "\n",
    "        pipe = Pipeline(steps)\n",
    "\n",
    "        return pipe\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        scoring: str = \"accuracy\",  # must be one of https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "        cv: int = 5,  # defaults to 5-fold CV\n",
    "        n_jobs: int = -1,  #  defaults to -1 to use all cores\n",
    "    ) -> tuple[list, list]:\n",
    "        \"\"\"Run GridSearchCV for all models on X_train and y_train of dataset.\n",
    "        Returns:\n",
    "            train_results: list of grid search results\n",
    "            grid_list: list of trained grid objects\n",
    "        \"\"\"\n",
    "\n",
    "        result_list = []\n",
    "        grid_list = []\n",
    "\n",
    "        for classifier, param_grid in zip(self.classifiers, self.param_grids):\n",
    "            print(f\"Starting grid search for {classifier}\")\n",
    "            grid = GridSearchCV(\n",
    "                estimator=self.pipeline,\n",
    "                param_grid=param_grid,\n",
    "                n_jobs=n_jobs,\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                verbose=1,\n",
    "            )\n",
    "\n",
    "            # with parallel_backend(\"multiprocessing\"):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                grid.fit(self.X_train, self.y_train)\n",
    "\n",
    "            result_list.append(pd.DataFrame.from_dict(grid.cv_results_))\n",
    "            grid_list.append(grid)\n",
    "\n",
    "        self.train_results = result_list\n",
    "        self.grid_list = grid_list\n",
    "\n",
    "        return self.train_results, self.grid_list\n",
    "\n",
    "    def get_top_5_train_results(self) -> list:\n",
    "        \"Return top 5 results for each grid\"\n",
    "        results = []\n",
    "        for result in self.train_results:\n",
    "            results.append(result.sort_values(\"rank_test_score\")[:5])\n",
    "        return results\n",
    "\n",
    "    def predict(self) -> pd.DataFrame:\n",
    "        \"\"\"Get predictions on the dataset's X_test from best estimators of GridSearchCV.\"\"\"\n",
    "        results = []\n",
    "        for classifier, grid in zip(self.classifiers, self.grid_list):\n",
    "            preds = grid.predict(self.X_test)\n",
    "            result = [\n",
    "                classifier,\n",
    "                grid.best_params_,\n",
    "                accuracy_score(self.y_test, preds),\n",
    "                recall_score(self.y_test, preds),\n",
    "                precision_score(self.y_test, preds),\n",
    "                f1_score(self.y_test, preds),\n",
    "            ]\n",
    "            results.append(result)\n",
    "\n",
    "        self.predict_results = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\n",
    "                \"classifier\",\n",
    "                \"best_params\",\n",
    "                \"accuracy\",\n",
    "                \"recall\",\n",
    "                \"precision\",\n",
    "                \"f1\",\n",
    "            ],\n",
    "        )\n",
    "        return self.predict_results\n",
    "\n",
    "    ## Unsupervised Learning ##\n",
    "\n",
    "    def pick_k(\n",
    "        self,\n",
    "        max_clusters: int = 10,  # max number of clusters to try out\n",
    "        pca_n_components: int = 50,  # number of components to reduce to in PCA\n",
    "    ) -> np.ndarray:  # PCA reduced X\n",
    "        \"\"\"Plot elbow and silohutte curves & print silohutte scores to help determine the ideal 'k' for Kmeans.\"\"\"\n",
    "       \n",
    "\n",
    "        # concat X\n",
    "        X = np.concatenate((self.X_train, self.X_test), axis=0)\n",
    "        assert (self.X_train.shape[0] + self.X_test.shape[0]) == X.shape[0]\n",
    "\n",
    "        if self.ohe:  # One Hot Encode X\n",
    "            ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "            X = ohe.fit_transform(X)\n",
    "\n",
    "        if self.scaler:  # Scale\n",
    "            scalr = StandardScaler()\n",
    "            X = scalr.fit_transform(X)\n",
    "\n",
    "        if self.pca:  # Dim Reduce X\n",
    "            pca = PCA(n_components=pca_n_components)\n",
    "            X = pca.fit_transform(X)\n",
    "\n",
    "        # visualize elbow plot\n",
    "        visualize_elbow(X, np.arange(2, max_clusters))\n",
    "        # visualize silhouette scores and plot\n",
    "        plot_silhouette_scores(max_clusters=max_clusters, X=X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def analyze_clusters(\n",
    "        self, \n",
    "        X_pca: np.ndarray,  # dim reduced X numpy ndarray\n",
    "        k: int,  # the chosen value of k for KMeans\n",
    "        random_state: int = 10  # random state for KMeans\n",
    "    ) -> None:\n",
    "        \"\"\"Perform KMeans clustering, print cluster counts and plot clusters from the result.\"\"\"\n",
    "\n",
    "        km = KMeans(n_clusters=k, random_state=random_state).fit(X_pca)\n",
    "        print(f\"Cluster counts: {Counter(km.labels_)}\")\n",
    "        visualize_clusters(km.labels_, X_pca)\n",
    "\n",
    "    ## Semi Supervised Learning ##\n",
    "\n",
    "    def run_label_spreading(\n",
    "        self,\n",
    "        pca_n_components: int = 50  # number of components to reduce to in PCA\n",
    "    ) -> None:\n",
    "        \"\"\"Run Lanel Spreading and print classification report.\"\"\"\n",
    "\n",
    "        # concat X\n",
    "        X = np.concatenate((self.X_train, self.X_test), axis=0)\n",
    "        assert (self.X_train.shape[0] + self.X_test.shape[0]) == X.shape[0]\n",
    "\n",
    "        # concat y\n",
    "        y = np.concatenate((self.y_train, np.full(self.y_test.shape, -1)), axis=0)\n",
    "        assert (self.y_train.shape[0] + self.y_test.shape[0]) == y.shape[0]\n",
    "\n",
    "        if self.ohe:  # One Hot Encode X\n",
    "            ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "            X = ohe.fit_transform(X)\n",
    "\n",
    "        if self.scaler:  # Scale\n",
    "            scalr = StandardScaler()\n",
    "            X = scalr.fit_transform(X)\n",
    "\n",
    "        if self.pca:  # Dim Reduce X\n",
    "            pca = PCA(n_components=pca_n_components)\n",
    "            X = pca.fit_transform(X)\n",
    "\n",
    "        # Run LableSpreading\n",
    "        lbl_spread = LabelSpreading(kernel=\"knn\", alpha=0.01)\n",
    "        lbl_spread.fit(X, y)\n",
    "        semi_sup_preds = lbl_spread.transduction_[self.X_train.shape[0] :]\n",
    "        assert semi_sup_preds.shape[0] == self.X_test.shape[0]\n",
    "\n",
    "        # print result\n",
    "        print(classification_report(self.y_test, semi_sup_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"Learner\" class=\"doc_header\"><code>class</code> <code>Learner</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>Learner</code>(**`X_train`**:`ndarray`, **`y_train`**:`ndarray`, **`X_test`**:`ndarray`, **`y_test`**:`ndarray`, **`ohe`**:`bool`=*`False`*, **`scaler`**:`bool`=*`False`*, **`pca`**:`bool`=*`True`*, **`pca_n_components`**:`int`=*`50`*, **`param_grids`**:`list`=*`None`*)\n",
       "\n",
       "```\n",
       "Class for training and prediction.\n",
       "```\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`X_train`**|`ndarray`||X_train numpy ndarray|\n",
       "|**`y_train`**|`ndarray`||y_train numpy ndarray|\n",
       "|**`X_test`**|`ndarray`||X_test numpy ndarray|\n",
       "|**`y_test`**|`ndarray`||y_test numpy ndarray|\n",
       "|**`ohe`**|`bool`|`False`|to use one hot encoding or not|\n",
       "|**`scaler`**|`bool`|`False`|to use standard scaling or not|\n",
       "|**`pca`**|`bool`|`True`|to use principal component analysis or not|\n",
       "|**`pca_n_components`**|`int`|`50`|PCA number of components|\n",
       "|**`param_grids`**|`list`|`None`|param_grid for grid search, if None - gets default grid from utils|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner, show_all_docments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.create_pipeline\" class=\"doc_header\"><code>Learner.create_pipeline</code><a href=\"__main__.py#L35\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.create_pipeline</code>()\n",
       "\n",
       "```\n",
       "Create and return pipeline\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.create_pipeline, show_all_docments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.train\" class=\"doc_header\"><code>Learner.train</code><a href=\"__main__.py#L51\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.train</code>(**`scoring`**:`str`=*`'accuracy'`*, **`cv`**:`int`=*`5`*, **`n_jobs`**:`int`=*`-1`*)\n",
       "\n",
       "```\n",
       "Run GridSearchCV for all models on X_train and y_train of dataset.\n",
       "Returns:\n",
       "    train_results: list of grid search results\n",
       "    grid_list: list of trained grid objects\n",
       "```\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`scoring`**|`str`|`accuracy`|must be one of https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter|\n",
       "|**`cv`**|`int`|`5`|defaults to 5-fold CV|\n",
       "|**`n_jobs`**|`int`|`-1`|defaults to -1 to use all cores|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.train, show_all_docments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.get_top_5_train_results\" class=\"doc_header\"><code>Learner.get_top_5_train_results</code><a href=\"__main__.py#L90\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.get_top_5_train_results</code>()\n",
       "\n",
       "```\n",
       "Return top 5 results for each grid\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.get_top_5_train_results, show_all_docments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.predict\" class=\"doc_header\"><code>Learner.predict</code><a href=\"__main__.py#L97\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.predict</code>()\n",
       "\n",
       "```\n",
       "Get predictions on the dataset's X_test from best estimators of GridSearchCV.\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.predict, show_all_docments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.pick_k\" class=\"doc_header\"><code>Learner.pick_k</code><a href=\"__main__.py#L127\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.pick_k</code>(**`max_clusters`**:`int`=*`10`*, **`pca_n_components`**:`int`=*`50`*)\n",
       "\n",
       "```\n",
       "Plot elbow and silohutte curves & print silohutte scores to help determine the ideal 'k' for Kmeans.\n",
       "```\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`max_clusters`**|`int`|`10`|max number of clusters to try out|\n",
       "|**`pca_n_components`**|`int`|`50`|number of components to reduce to in PCA|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.pick_k, show_all_docments=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `pick_k` method does the following to help determine the ideal k for KMeans: \n",
    " - It first concats X_train and X_test of this dataset into a single ndarray 'X'\n",
    " - then encodes X using OneHotEncoder\n",
    " - then sclaes X using StandardScaler\n",
    " - then dimensionality reduces X using PCA\n",
    " - then plots elbow & silhouette plots for X and prints silhouette scores, and returns the PCA-reduced X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.analyze_clusters\" class=\"doc_header\"><code>Learner.analyze_clusters</code><a href=\"__main__.py#L158\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.analyze_clusters</code>(**`X_pca`**:`ndarray`, **`k`**:`int`, **`random_state`**:`int`=*`10`*)\n",
       "\n",
       "```\n",
       "Perform KMeans clustering, print cluster counts and plot clusters from the result.\n",
       "```\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`X_pca`**|`ndarray`||dim reduced X numpy ndarray|\n",
       "|**`k`**|`int`||the chosen value of k for KMeans|\n",
       "|**`random_state`**|`int`|`10`|random state for KMeans|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.analyze_clusters, show_all_docments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.run_label_spreading\" class=\"doc_header\"><code>Learner.run_label_spreading</code><a href=\"__main__.py#L172\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.run_label_spreading</code>(**`pca_n_components`**:`int`=*`50`*)\n",
       "\n",
       "```\n",
       "Run Lanel Spreading and print classification report.\n",
       "```\n",
       "\n",
       "||Type|Default|Details|\n",
       "|---|---|---|---|\n",
       "|**`pca_n_components`**|`int`|`50`|number of components to reduce to in PCA|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.run_label_spreading, show_all_docments=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-announcement",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-candle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_basics.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_learn.ipynb.\n",
      "Converted 03_onehot.ipynb.\n",
      "Converted 04_lstm.ipynb.\n",
      "Converted 05_transformer.ipynb.\n",
      "Converted 99_utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('peptide')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed1fe5d2b8444463e19b00657a175b9cae3e282d841010e10efaed3bb2bffc1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
